{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining\n",
    "\n",
    "![miners](img/text-miners.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation discussion:\n",
    "\n",
    "Take 5 minutes and talk with your neighbor about:\n",
    " - What makes text mining hard?\n",
    " - How is it different than other types of analysis we've done?\n",
    " - What are some applications of text mining you've heard of? (or are _interested_ in?)t\n",
    " \n",
    "Be prepared to share out something your neighbor said with the group!\n",
    "\n",
    "***\n",
    "\n",
    "Report out!\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is text mining different? What is text?\n",
    "\n",
    "- Order the words from **SMALLEST** to **LARGEST** units\n",
    " - character\n",
    " - corpora\n",
    " - sentence\n",
    " - word\n",
    " - corpus\n",
    " - paragraph\n",
    " - document\n",
    "\n",
    "(after it is all organized)\n",
    "\n",
    "- Any disagreements about the terms used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Goal**: to internalize the steps, challenges, and methodology of text mining\n",
    "\n",
    "### Objectives\n",
    "- use tools we already have to vectorize some text\n",
    "- apply the `nltk` package to use some more robust tools to vectorize text\n",
    "- use TDIF to classify documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: First Exercise\n",
    "\n",
    "### Scenario:\n",
    "Imagine a time before topic tags on articles. We want to save time sorting through news articles from BBC news to only read the sports articles. We are going to try this using the tools we have.\n",
    "\n",
    "In the `text_examples`  are articles<br>\n",
    "(don't read them all! You'll ruin the surprise!)\n",
    "\n",
    "Here's what we are going to do:\n",
    "- read an article in to python as a long string\n",
    "- use `.split()` to convert the string to a list\n",
    "- fix any problems\n",
    "- create a bar chart of the top 20 most frequent words in your article\n",
    "- paste that bar chart [here](https://docs.google.com/presentation/d/1pTYQDlpFudghIM1rEDQNKgjubR77SgCUL4YtPjTmfeA/edit?usp=sharing)\n",
    "- Try to figure out whose articles are in the same topic as yours using the **bar charts only**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"text_examples/A.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_a = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_a[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_a_words = article_a.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_a_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "series_a = pd.Series(article_a_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_a.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "series_a.value_counts()[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the problem here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_a.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_a.apply(lambda x: x.lower()).value_counts()[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"remove_words.txt\", \"r\")\n",
    "remove = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list = remove.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter_series_a = series_a[~series_a.apply((lambda x: x.lower())).isin(remove_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter_series_a.apply((lambda x: x.lower())).value_counts()[:20].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter_series_a.apply((lambda x: x.lower())).value_counts()[:40].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What other things could we fix?\n",
    "\n",
    "- using regex, perhaps?\n",
    "- normalize for number of words in article?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do with your own articles if you haven't already!\n",
    "Find your group!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Text Vectorization, aka Bag of Words\n",
    "#### Steps:\n",
    "\n",
    "<img style=\"float: left\" src=\"./img/bag_of_word.jpg\" width=\"200\">\n",
    "\n",
    "![step by step](https://i.gifer.com/VxbJ.gif)\n",
    "\n",
    "1. make all lower case\n",
    "2. Remove punctuation, numbers, symbols, etc\n",
    "3. Remove stop words, perhaps develop custom stop words list\n",
    "4. Stemming/Lemmatization\n",
    "\n",
    "\n",
    "How was that process we just did?\n",
    "But what about tokenization? when's the best time to tokenize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK makes it easier!\n",
    "\n",
    "### _Natural Language Tool Kit_\n",
    "\n",
    "NLTK is its own python library. And of course, it has its own [documentation](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download() #for when you are bringing in files from gutenburg, etf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string\n",
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"text_examples/A.txt\", \"r\")\n",
    "article_a = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your article here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "article_a_tokens_raw = nltk.regexp_tokenize(article_a, pattern)\n",
    "print(article_a_tokens_raw[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_a_tokens = [i.lower() for i in article_a_tokens_raw]\n",
    "print(article_a_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "article_a_tokens_stopped = [w for w in article_a_tokens if not w in stop_words]\n",
    "print(article_a_tokens_stopped[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming / Lemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming - Porter Stemmer \n",
    "![porter](https://cdn.homebrewersassociation.org/wp-content/uploads/Baltic_Porter_Feature-600x800.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import *\n",
    "stemmer = PorterStemmer()\n",
    "example = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "          'plotted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singles = [stemmer.stem(example) for examp in example]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming - Snowball Stemmer\n",
    "![snowball](https://localtvwiti.files.wordpress.com/2018/08/gettyimages-936380496.jpg?quality=85&strip=all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "print(stemmer.stem(\"running\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter vs Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(SnowballStemmer(\"english\").stem(\"generously\"))\n",
    "print(SnowballStemmer(\"porter\").stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Snowball on metamorphesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_a_stemmed = [stemmer.stem(word) for word in article_a_tokens_stopped]\n",
    "print(article_a_stemmed[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Uses a corpus of words \"WordNet\"\n",
    "\n",
    "`from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()`\n",
    "\n",
    "\n",
    "Challenge of lemmatization:\n",
    "\n",
    "`wordnet_lemmatizer.lemmatize(word, pos=\"v\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is a short list of additional considerations when cleaning text:\n",
    "\n",
    "- Handling large documents and large collections of text documents that do not fit into memory.\n",
    "- Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "- Transliteration of characters from other languages into English.\n",
    "- Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "- Handling of domain specific words, phrases, and acronyms.\n",
    "- Handling or removing numbers, such as dates and amounts.\n",
    "- Locating and correcting common typos and misspellings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it with your article!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average word length in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(sum(map(len, article_a_stemmed))) / len(article_a_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of words in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(article_a_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_freqdist = FreqDist(article_a_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_freqdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "a_freqdist.plot(30,cumulative=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Create word frequency plot for your article\n",
    "\n",
    "Question:  Should any more stop words be added to the list given your plot results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_finder = BigramCollocationFinder.from_words(article_a_stemmed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you've all been waiting for \n",
    "\n",
    "![big deal](http://reddebtedstepchild.com/wp-content/uploads/2013/04/Big-deal-gif.gif)\n",
    "\n",
    "\n",
    "## Frequency distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a data frame that compares the documents\n",
    "\n",
    "**Puzzle**: how could you adapt the code below to allow you to compare documents and word counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hello  omg  pony  she  there  went  why\n",
      "0      1    0     0    0      1     0    1\n",
      "1      1    1     1    0      0     0    0\n",
      "2      0    1     0    1      1     1    0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = ['why hello there', 'omg hello pony', 'she went there? omg']\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(docs)\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing documents\n",
    "\n",
    "Why is the `CountVectorizer` not enough to adequately compare documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More math!\n",
    "\n",
    "$\n",
    "tf_{i,j} = \\text{number of occurences of } i \\text{ in}  j \\\\\n",
    "df_i = \\text{number of documents containing} i \\\\\n",
    "N = \\text{total number of documents}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)\n",
    "\n",
    "$\\begin{align}\n",
    " tf_{i,j} = \\dfrac{n_{i,j}}{\\displaystyle \\sum_k n_{i,j} }\n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "$\\begin{align}\n",
    "idf(w) = \\log \\dfrac{N}{df_t}\n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF score\n",
    "\n",
    "$ \\begin{align}\n",
    "w_{i,j} = tf_{i,j} \\times \\log \\dfrac{N}{df_i}\n",
    "\\end{align} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_a = ' '.join(article_a_stemmed)\n",
    "cleaned_b = ' '.join(FILL THIS)# need b article here\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "response = tfidf.fit_transform([cleaned_a, cleaned_b])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(response.toarray(), columns=tfidf.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXIT TICKET [HERE](https://forms.gle/WXAyoWjnFb7XdaJd9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _PREP FOR NEXT LECTURE_:\n",
    "\n",
    "Take a dataset of tweets and prepare it for next lecture.\n",
    "\n",
    "- explore a few tweets in your chosen dataset\n",
    "- practice cleaning them for text analysis\n",
    "- tokenizing, lower case, punctuation, stemming\n",
    "- then use `\" \".join()` to make each cleaned tweet back in to a string\n",
    "- save that cleaned string in a new dataframe\n",
    "\n",
    "Make a function that will do that cleaning for the whole dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Choices\n",
    "\n",
    "#### What words indicate a text might be biased?\n",
    "\n",
    "Politically Neutral vs Partisan dataset:\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://query.data.world/s/yexnb2tlowvuvq6dcnkex6jnlzzbmc')\n",
    "```\n",
    "\n",
    "#### Can you tell from tweets if someone was attending SXSW or Cochella?\n",
    "**2017 SXSW twitter**\n",
    "\n",
    "<img src=\"https://pmcvariety.files.wordpress.com/2019/10/sxsw.jpg?w=1000\" alt=\"sxsw\" style =\"text-align:center;width:200px;float:none\" >\n",
    "\n",
    "A collection of all tweets that mention #sxsw or @sxsw\n",
    "```\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://query.data.world/s/gt4rvczsuklcxymodiklrl7uq2vmyx')\n",
    "```\n",
    "\n",
    "\n",
    "**2015 Cochella twitter**\n",
    "\n",
    "<img src=\"https://consequenceofsound.net/wp-content/uploads/2019/12/Coachella-2020-lineup.png?w=800\" alt=\"cochella\" style =\"text-align:center;width:200px;float:none\" >\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://query.data.world/s/buoio54tyk6fg7gh7x3d7qclbmtqb2')\n",
    "```\n",
    "\n",
    "#### Trump vs Johnson\n",
    "\n",
    "**Boris Johnson Tweets** (as of last night)\n",
    "in the data folder!\n",
    "\n",
    "**Donal Trump Tweets**\n",
    "Same location!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "#### If you want to see TF, IDF, and TF-IDF from _scratch...\n",
    "\n",
    "![homemade](https://media2.giphy.com/media/LBZcXdG0eVBdK/giphy.gif?cid=3640f6095c2d7bb2526a424a4d97117c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordSet = set(arta_stemmed).union(set(artb_stemmed)) \n",
    "wordDictA = dict.fromkeys(wordSet, 0) \n",
    "wordDictB = dict.fromkeys(wordSet, 0) \n",
    "\n",
    "for word in arta_stemmed: \n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in artb_stemmed: \n",
    "    wordDictB[word]+=1    \n",
    "\n",
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict\n",
    "\n",
    "tfbowA = computeTF(wordDictA,arta_stemmed)\n",
    "tfbowB = computeTF(wordDictB,artb_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfbowA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    \"\"\" compute inverse doc freq for each doc in the docList\n",
    "    returns: IDF for each doc\n",
    "    \"\"\"\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    \"\"\"creates function for computing TFIDF\"\"\"\n",
    "    tfidf = {} # creates empty dictionary\n",
    "    for word, val in tfBow.items(): #starts a for loop using keys (word) and values from tfBow\n",
    "        tfidf[word] = val*idfs[word] #for each word in tfBow, the value is multiplied by the idfs for the word. \n",
    "                                        #The word and resulting computation are then added to the dictionary tfidf\n",
    "    return tfidf #returns the dictionary tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfBowA = computeTFIDF(tfbowA, idfs)\n",
    "tfidfBowB = computeTFIDF(tfbowB, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame([tfidfBowA, tfidfBowB])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
